{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d053f6b9",
   "metadata": {},
   "source": [
    "## Laboratorium 8\n",
    "\n",
    "\n",
    "## Detekcja obiektów za pomocą Faster-RCNN\n",
    "\n",
    "### Wprowadzenie\n",
    "\n",
    "Celem tej listy jest praktyczne zapoznanie się z działaniem dwuetapowych modeli do detekcji obiektów na przykładzie Faster R-CNN. Skorzystamy z gotowej implementacji modelu z pakietu [`torchvision`](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py). Jeżeli masz inny ulubiony model działający na podobnej zasadzie, możesz z niego skorzystać zamiast podanego. Podobnie implementacja - jeśli masz swoją ulubioną bibliotekę np. Detectron2, MMDetection, możesz z niej skorzystać.\n",
    "\n",
    "W zadaniu wykorzystany zostanie zbiór danych [_Chess Pieces Dataset_](https://public.roboflow.com/object-detection/chess-full) (autorstwa Roboflow, domena publiczna), ZIP z obrazami i anotacjami powinien być dołączony do instrukcji.\n",
    "\n",
    "Podczas realizacji tej listy większy nacisk położony zostanie na inferencję z użyciem Faster R-CNN niż na uczenie (które przeprowadzisz raz\\*). Kluczowe komponenty w tej architekturze (RPN i RoIHeads) można konfigurować bez ponownego uczenia, dlatego badania skupią się na ich strojeniu. Aby zrozumieć działanie modelu, konieczne będzie spojrzenie w jego głąb, włącznie z częściowym wykonaniem. W tym celu warto mieć na podorędziu kod źródłowy, w szczególności implementacje następujących klas:\n",
    "* `FasterRCNN`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/faster_rcnn.py\n",
    "* `GeneralizedRCNN`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/generalized_rcnn.py\n",
    "* `RegionProposalNetwork`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/rpn.py\n",
    "* `RoIHeads`: https://github.com/pytorch/vision/blob/main/torchvision/models/detection/roi_heads.py\n",
    "\n",
    "Dogłębne zrozumienie procedury uczenia modelu nie będzie wymagane, niemniej należy mieć ogólną świadomość jak ten proces przebiega i jakie funkcje kosztu są wykorzystywane. Użyjemy gotowej implementacji z submodułu [`references.detection`](https://github.com/pytorch/vision/blob/main/references/detection/train.py) w nieco uproszczonej wersji. Ponieważ ten moduł **nie** jest domyślnie instalowaną częścią pakietu `torchvision`, do instrukcji dołączono jego kod w nieznacznie zmodyfikowanej wersji (`references_detection.zip`).\n",
    "Jeśli ciekawią Cię szczegóły procesu uczenia, zachęcam do lektury [artykułu](https://arxiv.org/abs/1506.01497) i analizy kodu implementacji."
   ]
  },
  {
   "cell_type": "raw",
   "id": "4991badb",
   "metadata": {},
   "source": [
    "!pip install torchvision>=0.13 # jeśli nie posiadasz pakietu torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "886028c4",
   "metadata": {},
   "source": [
    "### Zadanie 0: Uczenie\n",
    "\n",
    "Krokiem \"zerowym\" będzie przygotowanie wstępnie nauczonego modelu i douczenie go na docelowym zbiorze.\n",
    "Podany zestaw hiperparametrów powinien dawać przyzwoite (niekoniecznie idealne) wyniki - jeśli chcesz, śmiało dobierz swoje własne; nie spędzaj na tym jednak zbyt wiele czasu.\n",
    "\n",
    "Twoim zadaniem jest nie tylko przeklikanie poniższych komórek, ale przynajmniej ogólne zrozumienie procesu uczenia (przejrzyj implementację `train_one_epoch`) i struktury modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37782fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torchvision\n",
    "import torchvision.models.detection as M\n",
    "from torchvision.io.image import read_image\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "from detection import coco_utils, presets, utils, transforms\n",
    "from detection.engine import train_one_epoch, evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8dd93d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(img_root:str, file_name:str, train:bool=True):\n",
    "  \"\"\"Reimplementacja analogicznej funkcji z pakietu references, rozwiązująca drobną niekompatybilność w zbiorze CPD\"\"\"\n",
    "  def fake_segmentation(image, target):\n",
    "    for obj in target['annotations']:\n",
    "      x, y, w, h = obj['bbox']\n",
    "      segm = [x, y, x+w, y, x+w, y+h, x, y+h]\n",
    "      obj['segmentation'] = [segm]\n",
    "    return image, target\n",
    "\n",
    "  tfs = transforms.Compose([\n",
    "    fake_segmentation,\n",
    "    coco_utils.ConvertCocoPolysToMask(),\n",
    "    presets.DetectionPresetTrain(data_augmentation='hflip') if train else presets.DetectionPresetEval(),\n",
    "    # jeśli chcesz dodać swoje własne augmentacje, możesz zrobić to tutaj\n",
    "  ])\n",
    "  ds = coco_utils.CocoDetection(img_root, file_name, transforms=tfs)\n",
    "  return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "523b9800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfiguracja hiperparametrów # NOTKA: te parametry sa byle jakie, powinny dzialac ale w razie czego mozna zmienic \n",
    "LR = 0.001 # powinno być dobrze dla 1 GPU\n",
    "WDECAY = 0.0001\n",
    "EPOCHS = 25\n",
    "VAL_FREQ = 5 # walidacja i checkpointowanie co N epok\n",
    "BATCH_SIZE = 2 # dobierz pod możliwości sprzętowe\n",
    "NUM_WORKERS = 8 # j/w\n",
    "NUM_CLASSES = 14\n",
    "DEVICE = 'cuda:0'\n",
    "DATASET_ROOT = 'chess/'\n",
    "OUTPUT_DIR = 'out2/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23d777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zaczytanie datasetów\n",
    "chess_train = get_dataset(os.path.join(DATASET_ROOT, 'train'), os.path.join(DATASET_ROOT, 'train/_annotations.coco.json'))\n",
    "chess_val = get_dataset(os.path.join(DATASET_ROOT, 'valid'), os.path.join(DATASET_ROOT, 'valid/_annotations.coco.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49517ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samplery i loadery\n",
    "train_sampler = torch.utils.data.RandomSampler(chess_train)\n",
    "train_batch_sampler = torch.utils.data.BatchSampler(train_sampler, BATCH_SIZE, drop_last=True)\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  chess_train, batch_sampler=train_batch_sampler, num_workers=NUM_WORKERS, collate_fn=utils.collate_fn\n",
    ")\n",
    "\n",
    "val_sampler = torch.utils.data.SequentialSampler(chess_val)\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "  chess_val, batch_size=1, sampler=val_sampler, num_workers=NUM_WORKERS, collate_fn=utils.collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36ffdca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skonstruowanie modelu; tworzymy w wersji dla 91 klas aby zainicjować wagi wstępnie nauczone na COCO...\n",
    "model = M.fasterrcnn_resnet50_fpn(weights=M.FasterRCNN_ResNet50_FPN_Weights.COCO_V1, num_classes=91).to(DEVICE)\n",
    "# ...po czym zastępujemy predyktor mniejszym, dostosowanym do naszego zbioru:\n",
    "model.roi_heads.box_predictor = M.faster_rcnn.FastRCNNPredictor(in_channels=1024, num_classes=NUM_CLASSES).to(DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b7079",
   "metadata": {},
   "outputs": [],
   "source": [
    "model # zwróć uwagę na strukturę Box Predictora (dlaczego tyle out_features?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3ab9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Zanim przejdziemy do uczenia pełnego modelu, wykonamy krótkie wstępne uczenie losowo zainicjowanego predyktora:\n",
    "train_one_epoch(\n",
    "  model=model,\n",
    "  optimizer=torch.optim.AdamW(model.roi_heads.box_predictor.parameters(), lr=LR, weight_decay=WDECAY),\n",
    "  data_loader=train_loader,\n",
    "  device=DEVICE,\n",
    "  epoch=0, print_freq=20, scaler=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d18b424",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uczenie pełnego modelu\n",
    "optimizer = torch.optim.AdamW(\n",
    "  [p for p in model.parameters() if p.requires_grad],\n",
    "  lr=LR,\n",
    "  weight_decay=WDECAY\n",
    ")\n",
    "lr_scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[10], gamma=0.1) # dobierz wartości jeśli trzeba\n",
    "\n",
    "start_time = time.time()\n",
    "for epoch in range(EPOCHS):\n",
    "    train_one_epoch(model, optimizer, train_loader, DEVICE, epoch, 20, None)\n",
    "    lr_scheduler.step()\n",
    "\n",
    "    # eval and checkpoint every VAL_FREQ epochs\n",
    "    if (epoch+1) % VAL_FREQ == 0:\n",
    "      checkpoint = {\n",
    "          \"model\": model.state_dict(),\n",
    "          \"optimizer\": optimizer.state_dict(),\n",
    "          \"lr_scheduler\": lr_scheduler.state_dict(),\n",
    "          \"epoch\": epoch,\n",
    "      }\n",
    "      utils.save_on_master(checkpoint, os.path.join(OUTPUT_DIR, f\"model_{epoch}.pth\"))\n",
    "      utils.save_on_master(checkpoint, os.path.join(OUTPUT_DIR, \"checkpoint.pth\"))\n",
    "      evaluate(model, val_loader, device=DEVICE)\n",
    "\n",
    "total_time = time.time() - start_time\n",
    "total_time_str = str(datetime.timedelta(seconds=int(total_time)))\n",
    "print(f\"Training time {total_time_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1765096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inferencja na zadanym obrazie\n",
    "preprocess = M.FasterRCNN_ResNet50_FPN_Weights.COCO_V1.transforms() # to wystarczy pobrać raz\n",
    "img = read_image(os.path.join(DATASET_ROOT, 'test/IMG_0159_JPG.rf.1cf4f243b5072d63e492711720df35f7.jpg'))\n",
    "batch = [preprocess(img).to(DEVICE)]\n",
    "prediction = model(batch)[0]\n",
    "# Rysowanie predykcji - wygodny gotowiec\n",
    "box = draw_bounding_boxes(\n",
    "  img,\n",
    "  boxes=prediction['boxes'],\n",
    "  labels=[chess_train.coco.cats[i.item()]['name'] for i in prediction['labels']],\n",
    "  colors='red',\n",
    "  width=4,\n",
    ")\n",
    "to_pil_image(box.detach()).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca90665f",
   "metadata": {},
   "source": [
    "---\n",
    "### Zadanie 1\n",
    "\n",
    "Zbadaj wpływ parametrów inferencji **głowic `RoIHeads`**, progu prawdopodobieństwa (`score_thresh`) i progu NMS (`nms_thresh`), na działanie modelu. Wykorzystaj funkcję `evaluate` aby zmierzyć zmianę jakości predykcji, ale przebadaj też efekty wizualnie, wyświetlając predykcje dla kilku obrazów ze zbioru walidacyjnego i kilku spoza zbioru (folder `wild`). _W finalnej wersji pozostaw tylko wybrane interesujące przykłady._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e99f52e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# niektore parametry modelu mozna zmieniac tylko po uczeniu i to wplywa tylko na inferencje i to jest super\n",
    "# tak chyba mamy robic tutaj, czyli nie uczymy modelu na nowo\n",
    "\n",
    "# slowo anchor jest wazne!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e58c4c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b5e7ee7",
   "metadata": {},
   "source": [
    "### Zadanie 2a\n",
    "\n",
    "Zwizualizuj propozycje rejonów wygenerowane przez RPN i porównaj z ostateczną predykcją.\n",
    "\n",
    "W tym celu konieczne będzie manualne wykonanie fragmentu metody `GeneralizedRCNN::forward` (patrz: [kod](https://github.com/pytorch/vision/blob/main/torchvision/models/detection/generalized_rcnn.py#L104)).\n",
    "Wszystkie fragmenty związane z uczeniem możesz rzecz jasna pominąć; chodzi o wyciągnięcie obiektu `proposals`.\n",
    "Nie zapomnij o wykonaniu powrotnej transformacji! (Po co?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89669823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# piszemy wlasnorecznie forward pass - kopiujemy fragmenty z implementacji powyzej i wywalamy predyktor\n",
    "# chcemy zobaczyc ile roboty robi rpn i ile na podstawie tego musi zrobic roihead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aebf536",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5480a0dd",
   "metadata": {},
   "source": [
    "### Zadanie 2b\n",
    "\n",
    "Zbadaj wpływ progu NMS _na etapie propozycji_ na jakość predykcji oraz czas ich uzyskania.\n",
    "Jak w poprzednich zadaniach, postaraj się nie ograniczyć tylko do pokazania metryk, ale pokaż wizualizacje (propozycji i predykcji) dla wybranych przykładów."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfdd0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zbior ma glownie obiekty large, co sprawia ze uczenie to powinno byc \"do pol godziny\"\n",
    "\n",
    "# wyniki przez to ze zbior nie jest bardzo duzy nie beda wygladaly idealnie\n",
    "# trzeba zrozumiec przyczyne wielokrotnych predykcji dla pewnych remionow\n",
    "\n",
    "\n",
    "# kazdy roi head jak dostaje propozycje to wypluwa k bounding boxow, gdzie k to liczba klas\n",
    "# mozemy dostac w tym samym rejonie dwie predykcje rozniace sie tylko klasa\n",
    "# nms nie patrzy na te klasy; wiec nie usunie dwoch predykcji podobnie pewnych dla danych klas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2623dd46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
